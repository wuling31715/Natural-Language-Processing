{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.1 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = list()\n",
    "        self.grads = list()\n",
    "        self.loss = None\n",
    "        self.y = None  \n",
    "        self.t = None \n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.2 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial L}{\\partial X} = (\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}... \\frac{\\partial L}{\\partial x_n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.3 Chain Lule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial X}{\\partial Z} = \\frac{\\partial X}{\\partial Y} \\frac{\\partial Y}{\\partial Z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.4 Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4.1 Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4.2 Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4.3 Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]]\n",
      "\n",
      "y:\n",
      "[[ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]\n",
      " [ 1.95483672  0.77489131  0.26713495  1.41444588 -0.48027678 -0.77894386\n",
      "   1.19416766 -0.45450741]]\n",
      "\n",
      "dy:\n",
      "[[ 0.16811354  1.00425853 -0.81644633  0.37138064  0.19343741  0.77807566\n",
      "   0.93497386  1.4104675 ]\n",
      " [ 0.51592855  0.29221703  1.29427547 -0.04570911  0.41562169  0.4281932\n",
      "  -0.273416   -0.61478619]\n",
      " [-0.20314406  1.34009325 -0.04240692  0.50495407 -0.5525257   1.71038557\n",
      "   0.31206376  0.39046404]\n",
      " [-0.4637005   0.48165103  1.02781081  1.06824933  0.06857098  0.27541538\n",
      "  -0.82722886 -1.28652529]\n",
      " [ 0.52761439 -0.0164192  -1.63196245  0.19749584  1.24280524  0.30886494\n",
      "  -0.87245213  1.66704968]\n",
      " [ 1.48592487 -0.55086372  0.41241884  0.16159452  0.59358603  0.23008831\n",
      "   0.42931779 -1.03735898]\n",
      " [-1.16283631 -0.1769532   1.19392984 -0.43159067 -2.09556952  1.14349907\n",
      "   0.05066295  0.27663112]]\n",
      "\n",
      "dx:\n",
      "[[ 0.86790049  2.37398372  1.43761927  1.82637461 -0.13407388  4.87452213\n",
      "  -0.24607862  0.80594189]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = 8\n",
    "N = 7\n",
    "x = np.random.randn(1, D)\n",
    "y = np.repeat(x, N, axis=0) # forward\n",
    "dy = np.random.randn(N, D) # gradient\n",
    "dx = np.sum(dy, axis=0, keepdims=True) # backward\n",
    "print(\"x:\\n{}\\n\".format(x))\n",
    "print(\"y:\\n{}\\n\".format(y))\n",
    "print(\"dy:\\n{}\\n\".format(dy))\n",
    "print(\"dx:\\n{}\\n\".format(dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4.4 Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[ 0.51508121  0.29003736  0.42900966 -0.49818843 -0.37028231 -1.44660693\n",
      "  -1.95696758  0.10767352]\n",
      " [-1.06022734 -1.06835591  0.01333767  0.82094468 -0.39445195 -0.6912514\n",
      "   0.22374962 -0.34125034]\n",
      " [ 0.43767875 -2.16004108  0.76272881  0.85319377  0.40110396  0.17212696\n",
      "   0.82244438  0.38426659]\n",
      " [ 1.23347096  1.31467389  0.50216413 -0.52750641 -0.37798191  0.27052371\n",
      "   0.03309953  0.18289856]\n",
      " [ 0.34142316  1.49135943  0.54762872  1.83318555  1.25830794 -0.58098627\n",
      "  -0.24456501 -0.25168757]\n",
      " [ 1.0913231   1.48800177  0.23052024  1.62312557 -0.36053911 -0.1086271\n",
      "  -1.29462005 -1.32395419]\n",
      " [ 0.0613258   0.95575305  0.57256438 -2.18571073  0.08369243 -0.26954473\n",
      "   1.02159193  0.10833606]]\n",
      "\n",
      "y:\n",
      "[[ 2.62007564  2.31142849  3.05795361  1.919044    0.23984905 -2.65436575\n",
      "  -1.39526718 -1.13371737]]\n",
      "\n",
      "dy:\n",
      "[[-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]]\n",
      "\n",
      "dx:\n",
      "[[-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]\n",
      " [-0.86079817 -0.06423127 -0.26390105 -0.94597227  1.18503516 -0.51447055\n",
      "  -0.25379368 -1.08943648]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = 8\n",
    "N = 7\n",
    "x = np.random.randn(N, D)\n",
    "y = np.sum(x, axis=0, keepdims=True) # forward\n",
    "dy = np.random.randn(1, D) # gradient\n",
    "dx = np.repeat(dy, N, axis=0) # backward\n",
    "print(\"x:\\n{}\\n\".format(x))\n",
    "print(\"y:\\n{}\\n\".format(y))\n",
    "print(\"dy:\\n{}\\n\".format(dy))\n",
    "print(\"dx:\\n{}\\n\".format(dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4.5 MatMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.5 Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.5.1 Sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = list()\n",
    "        self.grads = list()\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.5.2 Affine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.5.3 Softmax with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = list()\n",
    "        self.grads = list()\n",
    "        self.y = None  \n",
    "        self.t = None  \n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.6 Updating Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
